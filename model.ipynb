{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import _nnpack_available\n",
    "from torch.optim.swa_utils import SWALR\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo os arquivos de base de dados\n",
    "original_database = pd.read_csv('data/jm1.csv')\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "data = pd.concat([train_data, original_database], axis=0, ignore_index=True)\n",
    "train_data\n",
    "data_id = train_data.pop('id')\n",
    "\n",
    "# Coluna de rótulos\n",
    "label_name = 'defects'\n",
    "# Transformação do rótulo para 0 e 1\n",
    "data[label_name] = data[label_name].map({False: 0, True: 1})\n",
    "\n",
    "# Função para substituir valores \"descartáveis\" por 'NaN'\n",
    "def replace_with_nan(element):\n",
    "    if type(element) == str and not element.isalnum():\n",
    "        element = \"NaN\"\n",
    "    return element\n",
    "\n",
    "# Colunas que podem ter valores faltantes\n",
    "fix_cols = [\"uniq_Op\", \"uniq_Opnd\", \"total_Op\", \"total_Opnd\", \"branchCount\"]\n",
    "\n",
    "def fix_columns(df, cols):\n",
    "    # Cópia do dataframe\n",
    "    df_new = df.copy(deep = True)\n",
    "    # Aplicar a função 'replace_with_nan' para cara elemento de uma coluna\n",
    "    for col in cols:\n",
    "        df_new[col] = df_new[col].apply(replace_with_nan).astype(\"float\")\n",
    "    return df_new\n",
    "\n",
    "# Dataframe ajustado por 'replace_with_nan'\n",
    "data_fixed = fix_columns(data, fix_cols)\n",
    "\n",
    "data_fixed_drop = data_fixed.dropna() # Remover valores nulos\n",
    "data_fixed_drop = data_fixed_drop.drop(labels='id',axis=1) # Remover coluna 'id'\n",
    "\n",
    "# Remover alguns atributos\n",
    "drop_cols = [\"v(g)\", \"ev(g)\", \"l\", \"d\", \"i\", \"e\", \"t\"] \n",
    "def drop_columns(df, cols):\n",
    "    df_new = df.copy(deep=True)\n",
    "    df_new = df_new.drop(labels=cols, axis=1)\n",
    "    return df_new\n",
    "# Dataframe ajustado e com menos atributos\n",
    "data_fixed_drop = drop_columns(data_fixed_drop, drop_cols)\n",
    "\n",
    "targets = 'defects'\n",
    "train_set, test_set = train_test_split(data_fixed_drop,  \n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=30)\n",
    "train_y = train_set['defects'].copy()\n",
    "test_y = test_set['defects'].copy()\n",
    "train_X = train_set.drop(labels='defects', axis=1)\n",
    "test_X = test_set.drop(['defects'], axis=1)\n",
    "\n",
    "# Adicionando features de média para alguns atributos\n",
    "def add_feat(X):\n",
    "    df=X.copy()\n",
    "    df['mean_bnv']         = (df['n'] + df['v'] + df['b']) /3;\n",
    "    df['mean_uniqOpOpend'] = (df['uniq_Op'] + df['uniq_Opnd']) /2;\n",
    "    df['mean_totOpOpend']  = (df['total_Op'] + df['total_Opnd']) /2;\n",
    "    return df\n",
    "train_X = add_feat(train_X)\n",
    "test_data = add_feat(test_data)\n",
    "\n",
    "# Faz o ajuste de escala e transforma todos os atributos para float\n",
    "def scale(df):\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "    robust_df = scaler.fit_transform(df)\n",
    "    robust_df = pd.DataFrame(robust_df, columns =df.columns)\n",
    "    return robust_df\n",
    "train_X = scale(train_X)\n",
    "test_data =  scale(test_data)\n",
    "test_X = scale(test_X)\n",
    "\n",
    "# Transformando a base de treino em tensor float\n",
    "train_X_tensor = torch.FloatTensor(train_X.values)\n",
    "train_y_tensor = torch.LongTensor(train_y.values)\n",
    "# Transformando a base de treino em tensor float\n",
    "test_X_tensor = torch.FloatTensor(test_X.values)\n",
    "test_y_tensor = torch.LongTensor(test_y.values)\n",
    "# Datasets\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "val_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(17, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )        \n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    # Modo treino\n",
    "    model.train()\n",
    "    # Tamanho do dataloader\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute predicition and loss\n",
    "        prediction = model(X)\n",
    "        loss = loss_fn(prediction, y)\n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1)*len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Modo teste\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            prediction = model(X)\n",
    "            test_loss += loss_fn(prediction, y).item()\n",
    "            correct += (prediction.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {100*correct:>5f}%, Average loss: {test_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 50\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model = NeuralNetwork()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(model)\n",
    "swa_start = 38\n",
    "swa_scheduler = SWALR(optimizer, swa_lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------\")\n",
    "    if (t+1) >= swa_start:\n",
    "        print(\"SWA Starting.....\")\n",
    "    if (t+1) >= swa_start:\n",
    "        train_loop(train_dataloader, swa_model, loss_fn, optimizer)\n",
    "        test_loop(val_dataloader, swa_model, loss_fn)\n",
    "    else:\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loop(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
