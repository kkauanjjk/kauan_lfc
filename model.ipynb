{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import _nnpack_available\n",
    "from torch.optim.swa_utils import SWALR\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo os arquivos de base de dados\n",
    "original_database = pd.read_csv('data/jm1.csv')\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "data = pd.concat([train_data, original_database], axis=0, ignore_index=True)\n",
    "train_data\n",
    "data_id = train_data.pop('id')\n",
    "\n",
    "# Coluna de rótulos\n",
    "label_name = 'defects'\n",
    "# Transformação do rótulo para 0 e 1\n",
    "data[label_name] = data[label_name].map({False: 0, True: 1})\n",
    "\n",
    "# Função para substituir valores \"descartáveis\" por 'NaN'\n",
    "def replace_with_nan(element):\n",
    "    if type(element) == str and not element.isalnum():\n",
    "        element = \"NaN\"\n",
    "    return element\n",
    "\n",
    "# Colunas que podem ter valores faltantes\n",
    "fix_cols = [\"uniq_Op\", \"uniq_Opnd\", \"total_Op\", \"total_Opnd\", \"branchCount\"]\n",
    "\n",
    "def fix_columns(df, cols):\n",
    "    # Cópia do dataframe\n",
    "    df_new = df.copy(deep = True)\n",
    "    # Aplicar a função 'replace_with_nan' para cara elemento de uma coluna\n",
    "    for col in cols:\n",
    "        df_new[col] = df_new[col].apply(replace_with_nan).astype(\"float\")\n",
    "    return df_new\n",
    "\n",
    "# Dataframe ajustado por 'replace_with_nan'\n",
    "data_fixed = fix_columns(data, fix_cols)\n",
    "\n",
    "data_fixed_drop = data_fixed.dropna() # Remover valores nulos\n",
    "data_fixed_drop = data_fixed_drop.drop(labels='id',axis=1) # Remover coluna 'id'\n",
    "\n",
    "# Remover alguns atributos\n",
    "drop_cols = [\"v(g)\", \"ev(g)\", \"l\", \"d\", \"i\", \"e\", \"t\"] \n",
    "def drop_columns(df, cols):\n",
    "    df_new = df.copy(deep=True)\n",
    "    df_new = df_new.drop(labels=cols, axis=1)\n",
    "    return df_new\n",
    "# Dataframe ajustado e com menos atributos\n",
    "data_fixed_drop = drop_columns(data_fixed_drop, drop_cols)\n",
    "\n",
    "targets = 'defects'\n",
    "train_set, test_set = train_test_split(data_fixed_drop,  \n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=30)\n",
    "train_y = train_set['defects'].copy()\n",
    "test_y = test_set['defects'].copy()\n",
    "train_X = train_set.drop(labels='defects', axis=1)\n",
    "test_X = test_set.drop(['defects'], axis=1)\n",
    "\n",
    "# Adicionando features de média para alguns atributos\n",
    "def add_feat(X):\n",
    "    df=X.copy()\n",
    "    df['mean_bnv']         = (df['n'] + df['v'] + df['b']) /3;\n",
    "    df['mean_uniqOpOpend'] = (df['uniq_Op'] + df['uniq_Opnd']) /2;\n",
    "    df['mean_totOpOpend']  = (df['total_Op'] + df['total_Opnd']) /2;\n",
    "    return df\n",
    "train_X = add_feat(train_X)\n",
    "test_X = add_feat(test_X)\n",
    "test_data = add_feat(test_data)\n",
    "\n",
    "# Faz o ajuste de escala e transforma todos os atributos para float\n",
    "def scale(df):\n",
    "    scaler = preprocessing.RobustScaler()\n",
    "    robust_df = scaler.fit_transform(df)\n",
    "    robust_df = pd.DataFrame(robust_df, columns =df.columns)\n",
    "    return robust_df\n",
    "\n",
    "train_X = scale(train_X)\n",
    "test_data =  scale(test_data)\n",
    "test_X = scale(test_X)\n",
    "\n",
    "# Transformando a base de treino em tensor float\n",
    "train_X_tensor = torch.FloatTensor(train_X.values)\n",
    "train_y_tensor = torch.LongTensor(train_y.values)\n",
    "# Transformando a base de treino em tensor float\n",
    "test_X_tensor = torch.FloatTensor(test_X.values)\n",
    "test_y_tensor = torch.LongTensor(test_y.values)\n",
    "# Datasets\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "val_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc</th>\n",
       "      <th>iv(g)</th>\n",
       "      <th>n</th>\n",
       "      <th>v</th>\n",
       "      <th>b</th>\n",
       "      <th>lOCode</th>\n",
       "      <th>lOComment</th>\n",
       "      <th>lOBlank</th>\n",
       "      <th>locCodeAndComment</th>\n",
       "      <th>uniq_Op</th>\n",
       "      <th>uniq_Opnd</th>\n",
       "      <th>total_Op</th>\n",
       "      <th>total_Opnd</th>\n",
       "      <th>branchCount</th>\n",
       "      <th>mean_bnv</th>\n",
       "      <th>mean_uniqOpOpend</th>\n",
       "      <th>mean_totOpOpend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.634018</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.623332</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.593023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.392939</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.393432</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.441860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.206897</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.872093</td>\n",
       "      <td>8.533254</td>\n",
       "      <td>8.3125</td>\n",
       "      <td>6.421053</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.500</td>\n",
       "      <td>3.615385</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>7.257143</td>\n",
       "      <td>8.75</td>\n",
       "      <td>8.308724</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.895349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.103448</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.116279</td>\n",
       "      <td>-0.107039</td>\n",
       "      <td>-0.1250</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.153846</td>\n",
       "      <td>-0.137255</td>\n",
       "      <td>-0.114286</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.110451</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.139535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.172414</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.384615</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>-0.114286</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71229</th>\n",
       "      <td>-0.379310</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.593023</td>\n",
       "      <td>-0.497289</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-0.736842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>-0.923077</td>\n",
       "      <td>-0.588235</td>\n",
       "      <td>-0.571429</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.515796</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>-0.593023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71230</th>\n",
       "      <td>3.482759</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-0.593023</td>\n",
       "      <td>-0.497289</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-0.736842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>-0.923077</td>\n",
       "      <td>-0.588235</td>\n",
       "      <td>-0.571429</td>\n",
       "      <td>3.75</td>\n",
       "      <td>-0.515796</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>-0.593023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71231</th>\n",
       "      <td>1.655172</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.011628</td>\n",
       "      <td>2.175254</td>\n",
       "      <td>2.1250</td>\n",
       "      <td>2.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.057143</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.157340</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.011628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71232</th>\n",
       "      <td>-0.517241</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.418605</td>\n",
       "      <td>-0.386536</td>\n",
       "      <td>-0.3750</td>\n",
       "      <td>-0.736842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.615385</td>\n",
       "      <td>-0.411765</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.394663</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.418605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71233</th>\n",
       "      <td>1.275862</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>2.476744</td>\n",
       "      <td>2.702425</td>\n",
       "      <td>2.6250</td>\n",
       "      <td>1.684211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.125</td>\n",
       "      <td>2.307692</td>\n",
       "      <td>2.843137</td>\n",
       "      <td>1.971429</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.677046</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.476744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71234 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            loc     iv(g)         n         v       b    lOCode  lOComment  \\\n",
       "0      0.068966  0.000000  0.558140  0.634018  0.6250  0.105263        1.0   \n",
       "1      0.068966  0.333333  0.395349  0.392939  0.3750  0.157895        0.0   \n",
       "2      7.206897  8.000000  6.872093  8.533254  8.3125  6.421053       44.0   \n",
       "3     -0.103448  0.666667 -0.116279 -0.107039 -0.1250  0.105263        0.0   \n",
       "4     -0.172414 -0.333333  0.000000  0.001793  0.0000  0.000000        0.0   \n",
       "...         ...       ...       ...       ...     ...       ...        ...   \n",
       "71229 -0.379310  0.333333 -0.593023 -0.497289 -0.5000 -0.736842        0.0   \n",
       "71230  3.482759  3.000000 -0.593023 -0.497289 -0.5000 -0.736842        0.0   \n",
       "71231  1.655172  3.000000  2.011628  2.175254  2.1250  2.052632        0.0   \n",
       "71232 -0.517241 -0.333333 -0.418605 -0.386536 -0.3750 -0.736842        0.0   \n",
       "71233  1.275862 -0.333333  2.476744  2.702425  2.6250  1.684211        0.0   \n",
       "\n",
       "       lOBlank  locCodeAndComment  uniq_Op  uniq_Opnd  total_Op  total_Opnd  \\\n",
       "0         0.50                0.0   -0.250   0.846154  0.509804    0.742857   \n",
       "1         0.75                0.0   -0.125   0.461538  0.490196    0.400000   \n",
       "2         1.50                0.0    1.500   3.615385  6.666667    7.257143   \n",
       "3        -0.25                0.0   -0.250  -0.153846 -0.137255   -0.114286   \n",
       "4        -0.25                0.0    0.000  -0.384615  0.098039   -0.114286   \n",
       "...        ...                ...      ...        ...       ...         ...   \n",
       "71229    -0.50                0.0   -1.500  -0.923077 -0.588235   -0.571429   \n",
       "71230    -0.50                0.0   -1.500  -0.923077 -0.588235   -0.571429   \n",
       "71231     2.25                0.0    1.000   1.538462  2.000000    2.057143   \n",
       "71232    -0.25                0.0   -0.625  -0.615385 -0.411765   -0.400000   \n",
       "71233     2.00                0.0    1.125   2.307692  2.843137    1.971429   \n",
       "\n",
       "       branchCount  mean_bnv  mean_uniqOpOpend  mean_totOpOpend  \n",
       "0            -0.25  0.623332              0.50         0.593023  \n",
       "1             0.25  0.393432              0.30         0.441860  \n",
       "2             8.75  8.308724              3.00         6.895349  \n",
       "3             0.25 -0.110451             -0.15        -0.139535  \n",
       "4             0.00  0.000000             -0.20         0.000000  \n",
       "...            ...       ...               ...              ...  \n",
       "71229        -0.25 -0.515796             -1.15        -0.593023  \n",
       "71230         3.75 -0.515796             -1.15        -0.593023  \n",
       "71231         2.00  2.157340              1.45         2.011628  \n",
       "71232        -0.50 -0.394663             -0.60        -0.418605  \n",
       "71233         1.00  2.677046              2.00         2.476744  \n",
       "\n",
       "[71234 rows x 17 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc</th>\n",
       "      <th>iv(g)</th>\n",
       "      <th>n</th>\n",
       "      <th>v</th>\n",
       "      <th>b</th>\n",
       "      <th>lOCode</th>\n",
       "      <th>lOComment</th>\n",
       "      <th>lOBlank</th>\n",
       "      <th>locCodeAndComment</th>\n",
       "      <th>uniq_Op</th>\n",
       "      <th>uniq_Opnd</th>\n",
       "      <th>total_Op</th>\n",
       "      <th>total_Opnd</th>\n",
       "      <th>branchCount</th>\n",
       "      <th>mean_bnv</th>\n",
       "      <th>mean_uniqOpOpend</th>\n",
       "      <th>mean_totOpOpend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.517241</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.458824</td>\n",
       "      <td>-0.424258</td>\n",
       "      <td>-0.4375</td>\n",
       "      <td>-0.578947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>-0.538462</td>\n",
       "      <td>-0.450980</td>\n",
       "      <td>-0.470588</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.431683</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.068966</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.164706</td>\n",
       "      <td>1.205913</td>\n",
       "      <td>1.1250</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.307692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.411765</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.196126</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.152941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.379310</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.164706</td>\n",
       "      <td>-0.168551</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>-0.315789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.230769</td>\n",
       "      <td>-0.176471</td>\n",
       "      <td>-0.147059</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.170217</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.413793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.352941</td>\n",
       "      <td>-0.326511</td>\n",
       "      <td>-0.3125</td>\n",
       "      <td>-0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.538462</td>\n",
       "      <td>-0.313725</td>\n",
       "      <td>-0.382353</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.332743</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.034483</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.152941</td>\n",
       "      <td>-0.172211</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.230769</td>\n",
       "      <td>-0.137255</td>\n",
       "      <td>-0.176471</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.171478</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.164706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30524</th>\n",
       "      <td>1.482759</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1.388235</td>\n",
       "      <td>1.497379</td>\n",
       "      <td>1.4375</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.470588</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.476813</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.376471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30525</th>\n",
       "      <td>-0.275862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.058824</td>\n",
       "      <td>-0.052913</td>\n",
       "      <td>-0.0625</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.098039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.056185</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.070588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30526</th>\n",
       "      <td>-0.517241</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.352941</td>\n",
       "      <td>-0.339485</td>\n",
       "      <td>-0.3750</td>\n",
       "      <td>-0.473684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.615385</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.441176</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.343705</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.388235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30527</th>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.423529</td>\n",
       "      <td>0.423024</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.420354</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.458824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30528</th>\n",
       "      <td>-0.034483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.105882</td>\n",
       "      <td>-0.007191</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.157895</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>-0.117647</td>\n",
       "      <td>-0.088235</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.024904</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.117647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30529 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            loc     iv(g)         n         v       b    lOCode  lOComment  \\\n",
       "0     -0.517241 -0.333333 -0.458824 -0.424258 -0.4375 -0.578947        0.0   \n",
       "1      0.068966  1.333333  1.164706  1.205913  1.1250  0.263158        0.0   \n",
       "2     -0.379310 -0.333333 -0.164706 -0.168551 -0.1875 -0.315789        0.0   \n",
       "3     -0.413793  0.000000 -0.352941 -0.326511 -0.3125 -0.368421        0.0   \n",
       "4     -0.034483  0.333333 -0.152941 -0.172211 -0.1875  0.157895        0.0   \n",
       "...         ...       ...       ...       ...     ...       ...        ...   \n",
       "30524  1.482759  1.666667  1.388235  1.497379  1.4375  2.000000        4.0   \n",
       "30525 -0.275862  0.000000 -0.058824 -0.052913 -0.0625 -0.157895        0.0   \n",
       "30526 -0.517241 -0.333333 -0.352941 -0.339485 -0.3750 -0.473684        0.0   \n",
       "30527  0.275862  0.666667  0.423529  0.423024  0.3750  0.526316        0.0   \n",
       "30528 -0.034483  0.000000 -0.105882 -0.007191  0.0000 -0.157895        3.0   \n",
       "\n",
       "       lOBlank  locCodeAndComment  uniq_Op  uniq_Opnd  total_Op  total_Opnd  \\\n",
       "0        -0.25                0.0   -0.750  -0.538462 -0.450980   -0.470588   \n",
       "1         0.25                0.0    0.125   1.307692  1.000000    1.411765   \n",
       "2         0.25                0.0   -0.125  -0.230769 -0.176471   -0.147059   \n",
       "3        -0.25                0.0   -0.250  -0.538462 -0.313725   -0.382353   \n",
       "4         0.00                0.0   -0.250  -0.230769 -0.137255   -0.176471   \n",
       "...        ...                ...      ...        ...       ...         ...   \n",
       "30524     1.50                0.0    0.375   1.000000  1.333333    1.470588   \n",
       "30525    -0.25                0.0    0.250   0.000000 -0.098039    0.000000   \n",
       "30526    -0.50                0.0   -0.250  -0.615385 -0.333333   -0.441176   \n",
       "30527     2.75                0.0    0.250   0.461538  0.431373    0.529412   \n",
       "30528     0.50                0.0    0.125   0.461538 -0.117647   -0.088235   \n",
       "\n",
       "       branchCount  mean_bnv  mean_uniqOpOpend  mean_totOpOpend  \n",
       "0            -0.50 -0.431683             -0.65        -0.470588  \n",
       "1             0.75  1.196126              0.90         1.152941  \n",
       "2            -0.50 -0.170217             -0.20        -0.176471  \n",
       "3            -0.25 -0.332743             -0.45        -0.352941  \n",
       "4             0.00 -0.171478             -0.25        -0.164706  \n",
       "...            ...       ...               ...              ...  \n",
       "30524         1.25  1.476813              0.80         1.376471  \n",
       "30525         0.25 -0.056185              0.10        -0.070588  \n",
       "30526        -0.50 -0.343705             -0.50        -0.388235  \n",
       "30527         1.00  0.420354              0.40         0.458824  \n",
       "30528        -0.25 -0.024904              0.35        -0.117647  \n",
       "\n",
       "[30529 rows x 17 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(17, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )        \n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, swa_start, swa_model, scheduler, swa_scheduler, epoch, loss_fn, optimizer):\n",
    "    # Modo treino\n",
    "    model.train()\n",
    "    # Tamanho do dataloader\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute predicition and loss\n",
    "        prediction = model(X)\n",
    "        loss = loss_fn(prediction, y)\n",
    "        #Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if epoch >= swa_start :\n",
    "            swa_model.update_parameters(model)\n",
    "            swa_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1)*len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Modo teste\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            prediction = model(X)\n",
    "            test_loss += loss_fn(prediction, y).item()\n",
    "            correct += (prediction.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {100*correct:>5f}%, Average loss: {test_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "--------------\n",
      "loss: 0.749115 [   64/71234]\n",
      "loss: 0.623503 [ 6464/71234]\n",
      "loss: 0.543396 [12864/71234]\n",
      "loss: 0.500230 [19264/71234]\n",
      "loss: 0.539079 [25664/71234]\n",
      "loss: 0.602098 [32064/71234]\n",
      "loss: 0.573343 [38464/71234]\n",
      "loss: 0.523526 [44864/71234]\n",
      "loss: 0.462964 [51264/71234]\n",
      "loss: 0.489601 [57664/71234]\n",
      "loss: 0.617571 [64064/71234]\n",
      "loss: 0.479067 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 79.488355%, Average loss: 0.479623\n",
      "\n",
      "Epoch 2\n",
      "--------------\n",
      "loss: 0.491179 [   64/71234]\n",
      "loss: 0.453836 [ 6464/71234]\n",
      "loss: 0.388855 [12864/71234]\n",
      "loss: 0.386886 [19264/71234]\n",
      "loss: 0.466208 [25664/71234]\n",
      "loss: 0.602798 [32064/71234]\n",
      "loss: 0.557305 [38464/71234]\n",
      "loss: 0.487310 [44864/71234]\n",
      "loss: 0.422746 [51264/71234]\n",
      "loss: 0.438472 [57664/71234]\n",
      "loss: 0.568598 [64064/71234]\n",
      "loss: 0.465579 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.733073%, Average loss: 0.465248\n",
      "\n",
      "Epoch 3\n",
      "--------------\n",
      "loss: 0.521109 [   64/71234]\n",
      "loss: 0.429287 [ 6464/71234]\n",
      "loss: 0.363859 [12864/71234]\n",
      "loss: 0.354979 [19264/71234]\n",
      "loss: 0.465322 [25664/71234]\n",
      "loss: 0.616004 [32064/71234]\n",
      "loss: 0.547091 [38464/71234]\n",
      "loss: 0.472629 [44864/71234]\n",
      "loss: 0.419682 [51264/71234]\n",
      "loss: 0.434129 [57664/71234]\n",
      "loss: 0.595038 [64064/71234]\n",
      "loss: 0.467680 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.742900%, Average loss: 0.454668\n",
      "\n",
      "Epoch 4\n",
      "--------------\n",
      "loss: 0.511591 [   64/71234]\n",
      "loss: 0.400068 [ 6464/71234]\n",
      "loss: 0.361370 [12864/71234]\n",
      "loss: 0.353680 [19264/71234]\n",
      "loss: 0.410412 [25664/71234]\n",
      "loss: 0.614628 [32064/71234]\n",
      "loss: 0.546351 [38464/71234]\n",
      "loss: 0.463215 [44864/71234]\n",
      "loss: 0.422507 [51264/71234]\n",
      "loss: 0.420211 [57664/71234]\n",
      "loss: 0.567614 [64064/71234]\n",
      "loss: 0.466202 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.932883%, Average loss: 0.452396\n",
      "\n",
      "Epoch 5\n",
      "--------------\n",
      "loss: 0.515667 [   64/71234]\n",
      "loss: 0.401659 [ 6464/71234]\n",
      "loss: 0.362344 [12864/71234]\n",
      "loss: 0.346722 [19264/71234]\n",
      "loss: 0.404424 [25664/71234]\n",
      "loss: 0.627377 [32064/71234]\n",
      "loss: 0.542380 [38464/71234]\n",
      "loss: 0.459235 [44864/71234]\n",
      "loss: 0.429070 [51264/71234]\n",
      "loss: 0.415871 [57664/71234]\n",
      "loss: 0.568072 [64064/71234]\n",
      "loss: 0.468893 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.021324%, Average loss: 0.453403\n",
      "\n",
      "Epoch 6\n",
      "--------------\n",
      "loss: 0.526041 [   64/71234]\n",
      "loss: 0.390067 [ 6464/71234]\n",
      "loss: 0.361811 [12864/71234]\n",
      "loss: 0.343430 [19264/71234]\n",
      "loss: 0.393054 [25664/71234]\n",
      "loss: 0.615941 [32064/71234]\n",
      "loss: 0.538562 [38464/71234]\n",
      "loss: 0.455774 [44864/71234]\n",
      "loss: 0.430319 [51264/71234]\n",
      "loss: 0.413984 [57664/71234]\n",
      "loss: 0.576381 [64064/71234]\n",
      "loss: 0.468912 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.772380%, Average loss: 0.447588\n",
      "\n",
      "Epoch 7\n",
      "--------------\n",
      "loss: 0.506631 [   64/71234]\n",
      "loss: 0.389614 [ 6464/71234]\n",
      "loss: 0.364069 [12864/71234]\n",
      "loss: 0.346707 [19264/71234]\n",
      "loss: 0.396171 [25664/71234]\n",
      "loss: 0.628594 [32064/71234]\n",
      "loss: 0.542924 [38464/71234]\n",
      "loss: 0.455125 [44864/71234]\n",
      "loss: 0.434153 [51264/71234]\n",
      "loss: 0.410365 [57664/71234]\n",
      "loss: 0.546507 [64064/71234]\n",
      "loss: 0.469127 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.985293%, Average loss: 0.453100\n",
      "\n",
      "Epoch 8\n",
      "--------------\n",
      "loss: 0.528098 [   64/71234]\n",
      "loss: 0.393583 [ 6464/71234]\n",
      "loss: 0.360430 [12864/71234]\n",
      "loss: 0.340352 [19264/71234]\n",
      "loss: 0.392941 [25664/71234]\n",
      "loss: 0.626910 [32064/71234]\n",
      "loss: 0.533214 [38464/71234]\n",
      "loss: 0.454812 [44864/71234]\n",
      "loss: 0.435247 [51264/71234]\n",
      "loss: 0.416657 [57664/71234]\n",
      "loss: 0.574225 [64064/71234]\n",
      "loss: 0.471723 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.746176%, Average loss: 0.445573\n",
      "\n",
      "Epoch 9\n",
      "--------------\n",
      "loss: 0.500046 [   64/71234]\n",
      "loss: 0.384618 [ 6464/71234]\n",
      "loss: 0.366423 [12864/71234]\n",
      "loss: 0.347913 [19264/71234]\n",
      "loss: 0.392466 [25664/71234]\n",
      "loss: 0.627470 [32064/71234]\n",
      "loss: 0.536588 [38464/71234]\n",
      "loss: 0.453239 [44864/71234]\n",
      "loss: 0.434661 [51264/71234]\n",
      "loss: 0.407427 [57664/71234]\n",
      "loss: 0.543883 [64064/71234]\n",
      "loss: 0.469028 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.073733%, Average loss: 0.449627\n",
      "\n",
      "Epoch 10\n",
      "--------------\n",
      "loss: 0.517274 [   64/71234]\n",
      "loss: 0.393448 [ 6464/71234]\n",
      "loss: 0.359634 [12864/71234]\n",
      "loss: 0.342427 [19264/71234]\n",
      "loss: 0.393381 [25664/71234]\n",
      "loss: 0.630697 [32064/71234]\n",
      "loss: 0.531875 [38464/71234]\n",
      "loss: 0.453125 [44864/71234]\n",
      "loss: 0.439371 [51264/71234]\n",
      "loss: 0.413077 [57664/71234]\n",
      "loss: 0.567919 [64064/71234]\n",
      "loss: 0.471591 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.037702%, Average loss: 0.444844\n",
      "\n",
      "Epoch 11\n",
      "--------------\n",
      "loss: 0.504020 [   64/71234]\n",
      "loss: 0.383650 [ 6464/71234]\n",
      "loss: 0.359604 [12864/71234]\n",
      "loss: 0.348317 [19264/71234]\n",
      "loss: 0.391833 [25664/71234]\n",
      "loss: 0.623421 [32064/71234]\n",
      "loss: 0.536364 [38464/71234]\n",
      "loss: 0.451824 [44864/71234]\n",
      "loss: 0.439501 [51264/71234]\n",
      "loss: 0.404375 [57664/71234]\n",
      "loss: 0.550345 [64064/71234]\n",
      "loss: 0.468888 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.063906%, Average loss: 0.445299\n",
      "\n",
      "Epoch 12\n",
      "--------------\n",
      "loss: 0.505003 [   64/71234]\n",
      "loss: 0.392408 [ 6464/71234]\n",
      "loss: 0.358998 [12864/71234]\n",
      "loss: 0.347714 [19264/71234]\n",
      "loss: 0.393927 [25664/71234]\n",
      "loss: 0.631360 [32064/71234]\n",
      "loss: 0.535590 [38464/71234]\n",
      "loss: 0.452652 [44864/71234]\n",
      "loss: 0.444000 [51264/71234]\n",
      "loss: 0.403104 [57664/71234]\n",
      "loss: 0.554141 [64064/71234]\n",
      "loss: 0.470747 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.073733%, Average loss: 0.446448\n",
      "\n",
      "Epoch 13\n",
      "--------------\n",
      "loss: 0.511458 [   64/71234]\n",
      "loss: 0.385314 [ 6464/71234]\n",
      "loss: 0.359607 [12864/71234]\n",
      "loss: 0.346300 [19264/71234]\n",
      "loss: 0.391414 [25664/71234]\n",
      "loss: 0.618526 [32064/71234]\n",
      "loss: 0.534317 [38464/71234]\n",
      "loss: 0.451533 [44864/71234]\n",
      "loss: 0.442782 [51264/71234]\n",
      "loss: 0.403175 [57664/71234]\n",
      "loss: 0.562851 [64064/71234]\n",
      "loss: 0.469823 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.972190%, Average loss: 0.442316\n",
      "\n",
      "Epoch 14\n",
      "--------------\n",
      "loss: 0.495488 [   64/71234]\n",
      "loss: 0.387783 [ 6464/71234]\n",
      "loss: 0.360268 [12864/71234]\n",
      "loss: 0.349419 [19264/71234]\n",
      "loss: 0.396105 [25664/71234]\n",
      "loss: 0.630826 [32064/71234]\n",
      "loss: 0.541468 [38464/71234]\n",
      "loss: 0.452073 [44864/71234]\n",
      "loss: 0.447350 [51264/71234]\n",
      "loss: 0.401344 [57664/71234]\n",
      "loss: 0.541939 [64064/71234]\n",
      "loss: 0.469738 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.135969%, Average loss: 0.447460\n",
      "\n",
      "Epoch 15\n",
      "--------------\n",
      "loss: 0.514443 [   64/71234]\n",
      "loss: 0.390223 [ 6464/71234]\n",
      "loss: 0.357787 [12864/71234]\n",
      "loss: 0.343791 [19264/71234]\n",
      "loss: 0.392670 [25664/71234]\n",
      "loss: 0.625789 [32064/71234]\n",
      "loss: 0.533304 [38464/71234]\n",
      "loss: 0.452194 [44864/71234]\n",
      "loss: 0.444620 [51264/71234]\n",
      "loss: 0.406725 [57664/71234]\n",
      "loss: 0.567178 [64064/71234]\n",
      "loss: 0.472129 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.887025%, Average loss: 0.441244\n",
      "\n",
      "Epoch 16\n",
      "--------------\n",
      "loss: 0.489226 [   64/71234]\n",
      "loss: 0.384298 [ 6464/71234]\n",
      "loss: 0.363878 [12864/71234]\n",
      "loss: 0.350116 [19264/71234]\n",
      "loss: 0.394714 [25664/71234]\n",
      "loss: 0.630250 [32064/71234]\n",
      "loss: 0.538659 [38464/71234]\n",
      "loss: 0.451697 [44864/71234]\n",
      "loss: 0.447048 [51264/71234]\n",
      "loss: 0.400674 [57664/71234]\n",
      "loss: 0.541493 [64064/71234]\n",
      "loss: 0.468846 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.168725%, Average loss: 0.445790\n",
      "\n",
      "Epoch 17\n",
      "--------------\n",
      "loss: 0.510095 [   64/71234]\n",
      "loss: 0.392949 [ 6464/71234]\n",
      "loss: 0.356309 [12864/71234]\n",
      "loss: 0.344615 [19264/71234]\n",
      "loss: 0.393141 [25664/71234]\n",
      "loss: 0.632069 [32064/71234]\n",
      "loss: 0.534865 [38464/71234]\n",
      "loss: 0.451767 [44864/71234]\n",
      "loss: 0.447637 [51264/71234]\n",
      "loss: 0.407734 [57664/71234]\n",
      "loss: 0.563757 [64064/71234]\n",
      "loss: 0.470978 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.113040%, Average loss: 0.441363\n",
      "\n",
      "Epoch 18\n",
      "--------------\n",
      "loss: 0.496743 [   64/71234]\n",
      "loss: 0.383676 [ 6464/71234]\n",
      "loss: 0.357026 [12864/71234]\n",
      "loss: 0.350065 [19264/71234]\n",
      "loss: 0.392873 [25664/71234]\n",
      "loss: 0.626309 [32064/71234]\n",
      "loss: 0.540130 [38464/71234]\n",
      "loss: 0.451260 [44864/71234]\n",
      "loss: 0.447435 [51264/71234]\n",
      "loss: 0.400436 [57664/71234]\n",
      "loss: 0.545803 [64064/71234]\n",
      "loss: 0.468251 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.086835%, Average loss: 0.442567\n",
      "\n",
      "Epoch 19\n",
      "--------------\n",
      "loss: 0.501150 [   64/71234]\n",
      "loss: 0.392432 [ 6464/71234]\n",
      "loss: 0.355744 [12864/71234]\n",
      "loss: 0.348680 [19264/71234]\n",
      "loss: 0.393780 [25664/71234]\n",
      "loss: 0.633420 [32064/71234]\n",
      "loss: 0.539774 [38464/71234]\n",
      "loss: 0.452134 [44864/71234]\n",
      "loss: 0.450667 [51264/71234]\n",
      "loss: 0.400394 [57664/71234]\n",
      "loss: 0.552133 [64064/71234]\n",
      "loss: 0.470095 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.175276%, Average loss: 0.443477\n",
      "\n",
      "Epoch 20\n",
      "--------------\n",
      "loss: 0.506599 [   64/71234]\n",
      "loss: 0.385202 [ 6464/71234]\n",
      "loss: 0.357092 [12864/71234]\n",
      "loss: 0.347589 [19264/71234]\n",
      "loss: 0.391376 [25664/71234]\n",
      "loss: 0.620441 [32064/71234]\n",
      "loss: 0.539863 [38464/71234]\n",
      "loss: 0.451074 [44864/71234]\n",
      "loss: 0.448607 [51264/71234]\n",
      "loss: 0.400516 [57664/71234]\n",
      "loss: 0.559421 [64064/71234]\n",
      "loss: 0.468880 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.050804%, Average loss: 0.440183\n",
      "\n",
      "Epoch 21\n",
      "--------------\n",
      "loss: 0.492898 [   64/71234]\n",
      "loss: 0.388436 [ 6464/71234]\n",
      "loss: 0.357316 [12864/71234]\n",
      "loss: 0.349782 [19264/71234]\n",
      "loss: 0.394593 [25664/71234]\n",
      "loss: 0.631768 [32064/71234]\n",
      "loss: 0.547765 [38464/71234]\n",
      "loss: 0.451559 [44864/71234]\n",
      "loss: 0.450607 [51264/71234]\n",
      "loss: 0.399357 [57664/71234]\n",
      "loss: 0.540821 [64064/71234]\n",
      "loss: 0.468591 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.060631%, Average loss: 0.445037\n",
      "\n",
      "Epoch 22\n",
      "--------------\n",
      "loss: 0.511181 [   64/71234]\n",
      "loss: 0.389087 [ 6464/71234]\n",
      "loss: 0.356128 [12864/71234]\n",
      "loss: 0.344518 [19264/71234]\n",
      "loss: 0.391558 [25664/71234]\n",
      "loss: 0.625159 [32064/71234]\n",
      "loss: 0.539053 [38464/71234]\n",
      "loss: 0.452103 [44864/71234]\n",
      "loss: 0.448910 [51264/71234]\n",
      "loss: 0.404575 [57664/71234]\n",
      "loss: 0.565499 [64064/71234]\n",
      "loss: 0.471405 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.021324%, Average loss: 0.439319\n",
      "\n",
      "Epoch 23\n",
      "--------------\n",
      "loss: 0.487018 [   64/71234]\n",
      "loss: 0.384913 [ 6464/71234]\n",
      "loss: 0.362201 [12864/71234]\n",
      "loss: 0.350194 [19264/71234]\n",
      "loss: 0.393504 [25664/71234]\n",
      "loss: 0.630423 [32064/71234]\n",
      "loss: 0.545555 [38464/71234]\n",
      "loss: 0.451854 [44864/71234]\n",
      "loss: 0.450015 [51264/71234]\n",
      "loss: 0.399518 [57664/71234]\n",
      "loss: 0.540390 [64064/71234]\n",
      "loss: 0.467616 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.093387%, Average loss: 0.443814\n",
      "\n",
      "Epoch 24\n",
      "--------------\n",
      "loss: 0.507980 [   64/71234]\n",
      "loss: 0.392936 [ 6464/71234]\n",
      "loss: 0.355251 [12864/71234]\n",
      "loss: 0.344754 [19264/71234]\n",
      "loss: 0.391838 [25664/71234]\n",
      "loss: 0.631257 [32064/71234]\n",
      "loss: 0.540941 [38464/71234]\n",
      "loss: 0.451673 [44864/71234]\n",
      "loss: 0.450526 [51264/71234]\n",
      "loss: 0.407183 [57664/71234]\n",
      "loss: 0.562391 [64064/71234]\n",
      "loss: 0.469859 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.103213%, Average loss: 0.439593\n",
      "\n",
      "Epoch 25\n",
      "--------------\n",
      "loss: 0.494370 [   64/71234]\n",
      "loss: 0.384123 [ 6464/71234]\n",
      "loss: 0.356605 [12864/71234]\n",
      "loss: 0.349596 [19264/71234]\n",
      "loss: 0.391684 [25664/71234]\n",
      "loss: 0.626774 [32064/71234]\n",
      "loss: 0.545943 [38464/71234]\n",
      "loss: 0.451110 [44864/71234]\n",
      "loss: 0.450380 [51264/71234]\n",
      "loss: 0.400613 [57664/71234]\n",
      "loss: 0.543691 [64064/71234]\n",
      "loss: 0.467352 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.129418%, Average loss: 0.441116\n",
      "\n",
      "Epoch 26\n",
      "--------------\n",
      "loss: 0.500442 [   64/71234]\n",
      "loss: 0.393126 [ 6464/71234]\n",
      "loss: 0.355198 [12864/71234]\n",
      "loss: 0.348348 [19264/71234]\n",
      "loss: 0.392779 [25664/71234]\n",
      "loss: 0.632587 [32064/71234]\n",
      "loss: 0.545005 [38464/71234]\n",
      "loss: 0.452316 [44864/71234]\n",
      "loss: 0.451603 [51264/71234]\n",
      "loss: 0.400731 [57664/71234]\n",
      "loss: 0.551764 [64064/71234]\n",
      "loss: 0.468705 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.142520%, Average loss: 0.441659\n",
      "\n",
      "Epoch 27\n",
      "--------------\n",
      "loss: 0.504471 [   64/71234]\n",
      "loss: 0.385881 [ 6464/71234]\n",
      "loss: 0.356855 [12864/71234]\n",
      "loss: 0.347460 [19264/71234]\n",
      "loss: 0.391175 [25664/71234]\n",
      "loss: 0.620968 [32064/71234]\n",
      "loss: 0.544543 [38464/71234]\n",
      "loss: 0.450845 [44864/71234]\n",
      "loss: 0.450813 [51264/71234]\n",
      "loss: 0.401087 [57664/71234]\n",
      "loss: 0.555979 [64064/71234]\n",
      "loss: 0.467959 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.050804%, Average loss: 0.438937\n",
      "\n",
      "Epoch 28\n",
      "--------------\n",
      "loss: 0.492776 [   64/71234]\n",
      "loss: 0.389577 [ 6464/71234]\n",
      "loss: 0.356458 [12864/71234]\n",
      "loss: 0.349351 [19264/71234]\n",
      "loss: 0.393595 [25664/71234]\n",
      "loss: 0.632132 [32064/71234]\n",
      "loss: 0.552791 [38464/71234]\n",
      "loss: 0.451891 [44864/71234]\n",
      "loss: 0.451132 [51264/71234]\n",
      "loss: 0.399968 [57664/71234]\n",
      "loss: 0.539125 [64064/71234]\n",
      "loss: 0.467140 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.034426%, Average loss: 0.443451\n",
      "\n",
      "Epoch 29\n",
      "--------------\n",
      "loss: 0.510265 [   64/71234]\n",
      "loss: 0.389495 [ 6464/71234]\n",
      "loss: 0.355136 [12864/71234]\n",
      "loss: 0.344582 [19264/71234]\n",
      "loss: 0.391105 [25664/71234]\n",
      "loss: 0.624357 [32064/71234]\n",
      "loss: 0.543994 [38464/71234]\n",
      "loss: 0.451655 [44864/71234]\n",
      "loss: 0.450561 [51264/71234]\n",
      "loss: 0.404223 [57664/71234]\n",
      "loss: 0.560449 [64064/71234]\n",
      "loss: 0.469795 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.018048%, Average loss: 0.438112\n",
      "\n",
      "Epoch 30\n",
      "--------------\n",
      "loss: 0.487101 [   64/71234]\n",
      "loss: 0.386446 [ 6464/71234]\n",
      "loss: 0.361419 [12864/71234]\n",
      "loss: 0.349113 [19264/71234]\n",
      "loss: 0.391623 [25664/71234]\n",
      "loss: 0.629923 [32064/71234]\n",
      "loss: 0.550176 [38464/71234]\n",
      "loss: 0.451715 [44864/71234]\n",
      "loss: 0.451424 [51264/71234]\n",
      "loss: 0.401241 [57664/71234]\n",
      "loss: 0.537798 [64064/71234]\n",
      "loss: 0.466369 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.021324%, Average loss: 0.442537\n",
      "\n",
      "Epoch 31\n",
      "--------------\n",
      "loss: 0.507984 [   64/71234]\n",
      "loss: 0.393972 [ 6464/71234]\n",
      "loss: 0.354505 [12864/71234]\n",
      "loss: 0.344672 [19264/71234]\n",
      "loss: 0.391737 [25664/71234]\n",
      "loss: 0.631524 [32064/71234]\n",
      "loss: 0.544805 [38464/71234]\n",
      "loss: 0.451185 [44864/71234]\n",
      "loss: 0.450885 [51264/71234]\n",
      "loss: 0.408043 [57664/71234]\n",
      "loss: 0.559017 [64064/71234]\n",
      "loss: 0.468208 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.116316%, Average loss: 0.438319\n",
      "\n",
      "Epoch 32\n",
      "--------------\n",
      "loss: 0.493891 [   64/71234]\n",
      "loss: 0.385584 [ 6464/71234]\n",
      "loss: 0.356194 [12864/71234]\n",
      "loss: 0.348386 [19264/71234]\n",
      "loss: 0.390970 [25664/71234]\n",
      "loss: 0.625575 [32064/71234]\n",
      "loss: 0.549583 [38464/71234]\n",
      "loss: 0.450678 [44864/71234]\n",
      "loss: 0.451835 [51264/71234]\n",
      "loss: 0.401737 [57664/71234]\n",
      "loss: 0.541614 [64064/71234]\n",
      "loss: 0.465713 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.116316%, Average loss: 0.440149\n",
      "\n",
      "Epoch 33\n",
      "--------------\n",
      "loss: 0.502071 [   64/71234]\n",
      "loss: 0.394271 [ 6464/71234]\n",
      "loss: 0.353991 [12864/71234]\n",
      "loss: 0.347731 [19264/71234]\n",
      "loss: 0.391460 [25664/71234]\n",
      "loss: 0.631790 [32064/71234]\n",
      "loss: 0.548705 [38464/71234]\n",
      "loss: 0.452120 [44864/71234]\n",
      "loss: 0.451434 [51264/71234]\n",
      "loss: 0.402010 [57664/71234]\n",
      "loss: 0.549899 [64064/71234]\n",
      "loss: 0.466855 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.126142%, Average loss: 0.440420\n",
      "\n",
      "Epoch 34\n",
      "--------------\n",
      "loss: 0.505110 [   64/71234]\n",
      "loss: 0.387093 [ 6464/71234]\n",
      "loss: 0.356327 [12864/71234]\n",
      "loss: 0.346539 [19264/71234]\n",
      "loss: 0.390403 [25664/71234]\n",
      "loss: 0.619025 [32064/71234]\n",
      "loss: 0.548732 [38464/71234]\n",
      "loss: 0.449955 [44864/71234]\n",
      "loss: 0.451795 [51264/71234]\n",
      "loss: 0.402118 [57664/71234]\n",
      "loss: 0.552857 [64064/71234]\n",
      "loss: 0.465889 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.067182%, Average loss: 0.438216\n",
      "\n",
      "Epoch 35\n",
      "--------------\n",
      "loss: 0.494950 [   64/71234]\n",
      "loss: 0.391073 [ 6464/71234]\n",
      "loss: 0.355321 [12864/71234]\n",
      "loss: 0.348337 [19264/71234]\n",
      "loss: 0.392164 [25664/71234]\n",
      "loss: 0.629125 [32064/71234]\n",
      "loss: 0.555564 [38464/71234]\n",
      "loss: 0.451145 [44864/71234]\n",
      "loss: 0.451550 [51264/71234]\n",
      "loss: 0.401961 [57664/71234]\n",
      "loss: 0.538764 [64064/71234]\n",
      "loss: 0.465479 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.001671%, Average loss: 0.442222\n",
      "\n",
      "Epoch 36\n",
      "--------------\n",
      "loss: 0.511145 [   64/71234]\n",
      "loss: 0.390140 [ 6464/71234]\n",
      "loss: 0.354926 [12864/71234]\n",
      "loss: 0.344254 [19264/71234]\n",
      "loss: 0.391129 [25664/71234]\n",
      "loss: 0.621531 [32064/71234]\n",
      "loss: 0.547389 [38464/71234]\n",
      "loss: 0.450709 [44864/71234]\n",
      "loss: 0.450798 [51264/71234]\n",
      "loss: 0.404475 [57664/71234]\n",
      "loss: 0.557222 [64064/71234]\n",
      "loss: 0.468122 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.014773%, Average loss: 0.437330\n",
      "\n",
      "Epoch 37\n",
      "--------------\n",
      "loss: 0.489071 [   64/71234]\n",
      "loss: 0.387221 [ 6464/71234]\n",
      "loss: 0.360245 [12864/71234]\n",
      "loss: 0.348188 [19264/71234]\n",
      "loss: 0.390929 [25664/71234]\n",
      "loss: 0.626902 [32064/71234]\n",
      "loss: 0.553869 [38464/71234]\n",
      "loss: 0.450873 [44864/71234]\n",
      "loss: 0.451748 [51264/71234]\n",
      "loss: 0.402437 [57664/71234]\n",
      "loss: 0.536862 [64064/71234]\n",
      "loss: 0.464623 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.037702%, Average loss: 0.441756\n",
      "\n",
      "Epoch 38\n",
      "--------------\n",
      "loss: 0.509819 [   64/71234]\n",
      "loss: 0.394249 [ 6464/71234]\n",
      "loss: 0.353938 [12864/71234]\n",
      "loss: 0.344212 [19264/71234]\n",
      "loss: 0.391438 [25664/71234]\n",
      "loss: 0.628542 [32064/71234]\n",
      "loss: 0.548384 [38464/71234]\n",
      "loss: 0.450507 [44864/71234]\n",
      "loss: 0.450868 [51264/71234]\n",
      "loss: 0.408736 [57664/71234]\n",
      "loss: 0.556356 [64064/71234]\n",
      "loss: 0.466600 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.119591%, Average loss: 0.437573\n",
      "\n",
      "Epoch 39\n",
      "--------------\n",
      "loss: 0.495881 [   64/71234]\n",
      "loss: 0.385901 [ 6464/71234]\n",
      "loss: 0.355587 [12864/71234]\n",
      "loss: 0.347481 [19264/71234]\n",
      "loss: 0.389597 [25664/71234]\n",
      "loss: 0.622473 [32064/71234]\n",
      "loss: 0.553390 [38464/71234]\n",
      "loss: 0.450011 [44864/71234]\n",
      "loss: 0.452631 [51264/71234]\n",
      "loss: 0.402916 [57664/71234]\n",
      "loss: 0.539559 [64064/71234]\n",
      "loss: 0.463781 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.050804%, Average loss: 0.439742\n",
      "\n",
      "Epoch 40\n",
      "--------------\n",
      "loss: 0.504829 [   64/71234]\n",
      "loss: 0.394646 [ 6464/71234]\n",
      "loss: 0.353246 [12864/71234]\n",
      "loss: 0.346381 [19264/71234]\n",
      "loss: 0.391552 [25664/71234]\n",
      "loss: 0.629166 [32064/71234]\n",
      "loss: 0.552182 [38464/71234]\n",
      "loss: 0.451810 [44864/71234]\n",
      "loss: 0.451245 [51264/71234]\n",
      "loss: 0.403166 [57664/71234]\n",
      "loss: 0.548228 [64064/71234]\n",
      "loss: 0.464391 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.113040%, Average loss: 0.439925\n",
      "\n",
      "Epoch 41\n",
      "--------------\n",
      "loss: 0.507509 [   64/71234]\n",
      "loss: 0.387361 [ 6464/71234]\n",
      "loss: 0.355720 [12864/71234]\n",
      "loss: 0.345647 [19264/71234]\n",
      "loss: 0.389249 [25664/71234]\n",
      "loss: 0.616468 [32064/71234]\n",
      "loss: 0.553117 [38464/71234]\n",
      "loss: 0.449337 [44864/71234]\n",
      "loss: 0.452300 [51264/71234]\n",
      "loss: 0.403048 [57664/71234]\n",
      "loss: 0.549864 [64064/71234]\n",
      "loss: 0.463472 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.099938%, Average loss: 0.438057\n",
      "\n",
      "Epoch 42\n",
      "--------------\n",
      "loss: 0.498779 [   64/71234]\n",
      "loss: 0.391896 [ 6464/71234]\n",
      "loss: 0.353955 [12864/71234]\n",
      "loss: 0.347685 [19264/71234]\n",
      "loss: 0.390874 [25664/71234]\n",
      "loss: 0.626456 [32064/71234]\n",
      "loss: 0.559745 [38464/71234]\n",
      "loss: 0.451077 [44864/71234]\n",
      "loss: 0.452044 [51264/71234]\n",
      "loss: 0.403059 [57664/71234]\n",
      "loss: 0.537084 [64064/71234]\n",
      "loss: 0.463130 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.011497%, Average loss: 0.441688\n",
      "\n",
      "Epoch 43\n",
      "--------------\n",
      "loss: 0.513644 [   64/71234]\n",
      "loss: 0.390485 [ 6464/71234]\n",
      "loss: 0.354457 [12864/71234]\n",
      "loss: 0.343753 [19264/71234]\n",
      "loss: 0.390312 [25664/71234]\n",
      "loss: 0.618089 [32064/71234]\n",
      "loss: 0.551617 [38464/71234]\n",
      "loss: 0.450145 [44864/71234]\n",
      "loss: 0.450780 [51264/71234]\n",
      "loss: 0.404992 [57664/71234]\n",
      "loss: 0.553727 [64064/71234]\n",
      "loss: 0.466186 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.086835%, Average loss: 0.436857\n",
      "\n",
      "Epoch 44\n",
      "--------------\n",
      "loss: 0.491616 [   64/71234]\n",
      "loss: 0.388024 [ 6464/71234]\n",
      "loss: 0.359163 [12864/71234]\n",
      "loss: 0.346461 [19264/71234]\n",
      "loss: 0.390466 [25664/71234]\n",
      "loss: 0.624262 [32064/71234]\n",
      "loss: 0.559090 [38464/71234]\n",
      "loss: 0.450716 [44864/71234]\n",
      "loss: 0.452073 [51264/71234]\n",
      "loss: 0.403430 [57664/71234]\n",
      "loss: 0.534240 [64064/71234]\n",
      "loss: 0.462538 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.034426%, Average loss: 0.441328\n",
      "\n",
      "Epoch 45\n",
      "--------------\n",
      "loss: 0.511961 [   64/71234]\n",
      "loss: 0.394188 [ 6464/71234]\n",
      "loss: 0.352734 [12864/71234]\n",
      "loss: 0.343499 [19264/71234]\n",
      "loss: 0.390874 [25664/71234]\n",
      "loss: 0.625759 [32064/71234]\n",
      "loss: 0.552852 [38464/71234]\n",
      "loss: 0.449853 [44864/71234]\n",
      "loss: 0.450496 [51264/71234]\n",
      "loss: 0.409042 [57664/71234]\n",
      "loss: 0.552498 [64064/71234]\n",
      "loss: 0.465175 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.142520%, Average loss: 0.437039\n",
      "\n",
      "Epoch 46\n",
      "--------------\n",
      "loss: 0.497062 [   64/71234]\n",
      "loss: 0.386981 [ 6464/71234]\n",
      "loss: 0.354315 [12864/71234]\n",
      "loss: 0.346346 [19264/71234]\n",
      "loss: 0.388431 [25664/71234]\n",
      "loss: 0.620229 [32064/71234]\n",
      "loss: 0.558582 [38464/71234]\n",
      "loss: 0.449815 [44864/71234]\n",
      "loss: 0.452317 [51264/71234]\n",
      "loss: 0.403657 [57664/71234]\n",
      "loss: 0.536252 [64064/71234]\n",
      "loss: 0.461446 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.034426%, Average loss: 0.439637\n",
      "\n",
      "Epoch 47\n",
      "--------------\n",
      "loss: 0.507053 [   64/71234]\n",
      "loss: 0.394957 [ 6464/71234]\n",
      "loss: 0.351694 [12864/71234]\n",
      "loss: 0.345658 [19264/71234]\n",
      "loss: 0.390077 [25664/71234]\n",
      "loss: 0.626919 [32064/71234]\n",
      "loss: 0.556772 [38464/71234]\n",
      "loss: 0.451490 [44864/71234]\n",
      "loss: 0.450730 [51264/71234]\n",
      "loss: 0.403797 [57664/71234]\n",
      "loss: 0.545210 [64064/71234]\n",
      "loss: 0.462862 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.031151%, Average loss: 0.439311\n",
      "\n",
      "Epoch 48\n",
      "--------------\n",
      "loss: 0.508793 [   64/71234]\n",
      "loss: 0.387283 [ 6464/71234]\n",
      "loss: 0.353718 [12864/71234]\n",
      "loss: 0.345127 [19264/71234]\n",
      "loss: 0.387938 [25664/71234]\n",
      "loss: 0.614132 [32064/71234]\n",
      "loss: 0.557827 [38464/71234]\n",
      "loss: 0.448714 [44864/71234]\n",
      "loss: 0.452120 [51264/71234]\n",
      "loss: 0.403663 [57664/71234]\n",
      "loss: 0.545525 [64064/71234]\n",
      "loss: 0.461438 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.034426%, Average loss: 0.438026\n",
      "\n",
      "Epoch 49\n",
      "--------------\n",
      "loss: 0.501221 [   64/71234]\n",
      "loss: 0.392773 [ 6464/71234]\n",
      "loss: 0.351905 [12864/71234]\n",
      "loss: 0.346343 [19264/71234]\n",
      "loss: 0.390206 [25664/71234]\n",
      "loss: 0.624306 [32064/71234]\n",
      "loss: 0.563906 [38464/71234]\n",
      "loss: 0.450714 [44864/71234]\n",
      "loss: 0.451764 [51264/71234]\n",
      "loss: 0.403537 [57664/71234]\n",
      "loss: 0.534666 [64064/71234]\n",
      "loss: 0.461462 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.011497%, Average loss: 0.441283\n",
      "\n",
      "Epoch 50\n",
      "--------------\n",
      "loss: 0.514751 [   64/71234]\n",
      "loss: 0.390784 [ 6464/71234]\n",
      "loss: 0.352187 [12864/71234]\n",
      "loss: 0.343318 [19264/71234]\n",
      "loss: 0.389156 [25664/71234]\n",
      "loss: 0.614340 [32064/71234]\n",
      "loss: 0.556140 [38464/71234]\n",
      "loss: 0.449418 [44864/71234]\n",
      "loss: 0.450738 [51264/71234]\n",
      "loss: 0.405149 [57664/71234]\n",
      "loss: 0.549503 [64064/71234]\n",
      "loss: 0.464878 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.122867%, Average loss: 0.436538\n",
      "\n",
      "Epoch 51\n",
      "--------------\n",
      "loss: 0.493488 [   64/71234]\n",
      "loss: 0.388041 [ 6464/71234]\n",
      "loss: 0.356424 [12864/71234]\n",
      "loss: 0.346571 [19264/71234]\n",
      "loss: 0.388415 [25664/71234]\n",
      "loss: 0.621271 [32064/71234]\n",
      "loss: 0.564563 [38464/71234]\n",
      "loss: 0.450531 [44864/71234]\n",
      "loss: 0.452379 [51264/71234]\n",
      "loss: 0.403802 [57664/71234]\n",
      "loss: 0.530634 [64064/71234]\n",
      "loss: 0.460606 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.021324%, Average loss: 0.441070\n",
      "\n",
      "Epoch 52\n",
      "--------------\n",
      "loss: 0.513644 [   64/71234]\n",
      "loss: 0.393922 [ 6464/71234]\n",
      "loss: 0.350398 [12864/71234]\n",
      "loss: 0.343340 [19264/71234]\n",
      "loss: 0.389187 [25664/71234]\n",
      "loss: 0.622044 [32064/71234]\n",
      "loss: 0.557554 [38464/71234]\n",
      "loss: 0.449125 [44864/71234]\n",
      "loss: 0.450484 [51264/71234]\n",
      "loss: 0.409409 [57664/71234]\n",
      "loss: 0.548038 [64064/71234]\n",
      "loss: 0.464156 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.129418%, Average loss: 0.436655\n",
      "\n",
      "Epoch 53\n",
      "--------------\n",
      "loss: 0.497747 [   64/71234]\n",
      "loss: 0.387066 [ 6464/71234]\n",
      "loss: 0.352197 [12864/71234]\n",
      "loss: 0.345511 [19264/71234]\n",
      "loss: 0.386986 [25664/71234]\n",
      "loss: 0.617379 [32064/71234]\n",
      "loss: 0.563619 [38464/71234]\n",
      "loss: 0.449335 [44864/71234]\n",
      "loss: 0.452354 [51264/71234]\n",
      "loss: 0.404168 [57664/71234]\n",
      "loss: 0.531793 [64064/71234]\n",
      "loss: 0.459885 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.027875%, Average loss: 0.439467\n",
      "\n",
      "Epoch 54\n",
      "--------------\n",
      "loss: 0.508841 [   64/71234]\n",
      "loss: 0.394798 [ 6464/71234]\n",
      "loss: 0.349301 [12864/71234]\n",
      "loss: 0.344837 [19264/71234]\n",
      "loss: 0.388753 [25664/71234]\n",
      "loss: 0.624211 [32064/71234]\n",
      "loss: 0.561145 [38464/71234]\n",
      "loss: 0.450835 [44864/71234]\n",
      "loss: 0.450487 [51264/71234]\n",
      "loss: 0.404493 [57664/71234]\n",
      "loss: 0.541850 [64064/71234]\n",
      "loss: 0.461659 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.060631%, Average loss: 0.438741\n",
      "\n",
      "Epoch 55\n",
      "--------------\n",
      "loss: 0.509407 [   64/71234]\n",
      "loss: 0.386972 [ 6464/71234]\n",
      "loss: 0.351444 [12864/71234]\n",
      "loss: 0.344712 [19264/71234]\n",
      "loss: 0.386285 [25664/71234]\n",
      "loss: 0.611251 [32064/71234]\n",
      "loss: 0.563151 [38464/71234]\n",
      "loss: 0.447955 [44864/71234]\n",
      "loss: 0.451949 [51264/71234]\n",
      "loss: 0.404108 [57664/71234]\n",
      "loss: 0.540520 [64064/71234]\n",
      "loss: 0.459865 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.060631%, Average loss: 0.437864\n",
      "\n",
      "Epoch 56\n",
      "--------------\n",
      "loss: 0.503165 [   64/71234]\n",
      "loss: 0.393220 [ 6464/71234]\n",
      "loss: 0.349214 [12864/71234]\n",
      "loss: 0.345961 [19264/71234]\n",
      "loss: 0.388584 [25664/71234]\n",
      "loss: 0.622069 [32064/71234]\n",
      "loss: 0.568225 [38464/71234]\n",
      "loss: 0.449890 [44864/71234]\n",
      "loss: 0.451369 [51264/71234]\n",
      "loss: 0.403963 [57664/71234]\n",
      "loss: 0.531744 [64064/71234]\n",
      "loss: 0.459984 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.027875%, Average loss: 0.440794\n",
      "\n",
      "Epoch 57\n",
      "--------------\n",
      "loss: 0.515441 [   64/71234]\n",
      "loss: 0.390525 [ 6464/71234]\n",
      "loss: 0.349955 [12864/71234]\n",
      "loss: 0.342907 [19264/71234]\n",
      "loss: 0.387619 [25664/71234]\n",
      "loss: 0.611002 [32064/71234]\n",
      "loss: 0.560781 [38464/71234]\n",
      "loss: 0.448148 [44864/71234]\n",
      "loss: 0.450205 [51264/71234]\n",
      "loss: 0.405597 [57664/71234]\n",
      "loss: 0.545568 [64064/71234]\n",
      "loss: 0.463212 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.096662%, Average loss: 0.436336\n",
      "\n",
      "Epoch 58\n",
      "--------------\n",
      "loss: 0.495162 [   64/71234]\n",
      "loss: 0.388713 [ 6464/71234]\n",
      "loss: 0.353481 [12864/71234]\n",
      "loss: 0.345675 [19264/71234]\n",
      "loss: 0.387338 [25664/71234]\n",
      "loss: 0.618997 [32064/71234]\n",
      "loss: 0.568956 [38464/71234]\n",
      "loss: 0.449798 [44864/71234]\n",
      "loss: 0.451993 [51264/71234]\n",
      "loss: 0.404450 [57664/71234]\n",
      "loss: 0.526995 [64064/71234]\n",
      "loss: 0.459422 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.014773%, Average loss: 0.440757\n",
      "\n",
      "Epoch 59\n",
      "--------------\n",
      "loss: 0.514136 [   64/71234]\n",
      "loss: 0.393624 [ 6464/71234]\n",
      "loss: 0.348290 [12864/71234]\n",
      "loss: 0.342924 [19264/71234]\n",
      "loss: 0.387718 [25664/71234]\n",
      "loss: 0.619290 [32064/71234]\n",
      "loss: 0.561233 [38464/71234]\n",
      "loss: 0.448277 [44864/71234]\n",
      "loss: 0.449675 [51264/71234]\n",
      "loss: 0.409724 [57664/71234]\n",
      "loss: 0.544659 [64064/71234]\n",
      "loss: 0.463685 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.145796%, Average loss: 0.436358\n",
      "\n",
      "Epoch 60\n",
      "--------------\n",
      "loss: 0.497846 [   64/71234]\n",
      "loss: 0.386662 [ 6464/71234]\n",
      "loss: 0.350397 [12864/71234]\n",
      "loss: 0.344789 [19264/71234]\n",
      "loss: 0.385191 [25664/71234]\n",
      "loss: 0.615405 [32064/71234]\n",
      "loss: 0.567307 [38464/71234]\n",
      "loss: 0.449034 [44864/71234]\n",
      "loss: 0.451666 [51264/71234]\n",
      "loss: 0.404849 [57664/71234]\n",
      "loss: 0.528270 [64064/71234]\n",
      "loss: 0.459024 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.054080%, Average loss: 0.439318\n",
      "\n",
      "Epoch 61\n",
      "--------------\n",
      "loss: 0.509382 [   64/71234]\n",
      "loss: 0.394753 [ 6464/71234]\n",
      "loss: 0.347388 [12864/71234]\n",
      "loss: 0.344272 [19264/71234]\n",
      "loss: 0.387185 [25664/71234]\n",
      "loss: 0.621848 [32064/71234]\n",
      "loss: 0.564237 [38464/71234]\n",
      "loss: 0.450309 [44864/71234]\n",
      "loss: 0.449720 [51264/71234]\n",
      "loss: 0.405498 [57664/71234]\n",
      "loss: 0.539509 [64064/71234]\n",
      "loss: 0.460795 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.096662%, Average loss: 0.438284\n",
      "\n",
      "Epoch 62\n",
      "--------------\n",
      "loss: 0.509058 [   64/71234]\n",
      "loss: 0.386837 [ 6464/71234]\n",
      "loss: 0.349528 [12864/71234]\n",
      "loss: 0.344020 [19264/71234]\n",
      "loss: 0.384709 [25664/71234]\n",
      "loss: 0.609417 [32064/71234]\n",
      "loss: 0.566453 [38464/71234]\n",
      "loss: 0.447650 [44864/71234]\n",
      "loss: 0.451390 [51264/71234]\n",
      "loss: 0.404841 [57664/71234]\n",
      "loss: 0.536523 [64064/71234]\n",
      "loss: 0.458641 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.109764%, Average loss: 0.437824\n",
      "\n",
      "Epoch 63\n",
      "--------------\n",
      "loss: 0.504210 [   64/71234]\n",
      "loss: 0.394366 [ 6464/71234]\n",
      "loss: 0.347421 [12864/71234]\n",
      "loss: 0.344855 [19264/71234]\n",
      "loss: 0.387836 [25664/71234]\n",
      "loss: 0.619853 [32064/71234]\n",
      "loss: 0.570345 [38464/71234]\n",
      "loss: 0.449621 [44864/71234]\n",
      "loss: 0.450553 [51264/71234]\n",
      "loss: 0.404425 [57664/71234]\n",
      "loss: 0.530088 [64064/71234]\n",
      "loss: 0.459078 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.034426%, Average loss: 0.440458\n",
      "\n",
      "Epoch 64\n",
      "--------------\n",
      "loss: 0.515207 [   64/71234]\n",
      "loss: 0.389917 [ 6464/71234]\n",
      "loss: 0.348040 [12864/71234]\n",
      "loss: 0.342615 [19264/71234]\n",
      "loss: 0.385986 [25664/71234]\n",
      "loss: 0.608602 [32064/71234]\n",
      "loss: 0.563344 [38464/71234]\n",
      "loss: 0.448000 [44864/71234]\n",
      "loss: 0.449885 [51264/71234]\n",
      "loss: 0.406283 [57664/71234]\n",
      "loss: 0.542507 [64064/71234]\n",
      "loss: 0.462103 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.099938%, Average loss: 0.436222\n",
      "\n",
      "Epoch 65\n",
      "--------------\n",
      "loss: 0.496009 [   64/71234]\n",
      "loss: 0.388904 [ 6464/71234]\n",
      "loss: 0.350393 [12864/71234]\n",
      "loss: 0.345003 [19264/71234]\n",
      "loss: 0.385587 [25664/71234]\n",
      "loss: 0.616970 [32064/71234]\n",
      "loss: 0.570925 [38464/71234]\n",
      "loss: 0.449789 [44864/71234]\n",
      "loss: 0.451424 [51264/71234]\n",
      "loss: 0.405029 [57664/71234]\n",
      "loss: 0.524946 [64064/71234]\n",
      "loss: 0.458549 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.001671%, Average loss: 0.440568\n",
      "\n",
      "Epoch 66\n",
      "--------------\n",
      "loss: 0.513935 [   64/71234]\n",
      "loss: 0.392743 [ 6464/71234]\n",
      "loss: 0.345850 [12864/71234]\n",
      "loss: 0.342369 [19264/71234]\n",
      "loss: 0.386690 [25664/71234]\n",
      "loss: 0.616999 [32064/71234]\n",
      "loss: 0.563083 [38464/71234]\n",
      "loss: 0.448603 [44864/71234]\n",
      "loss: 0.448748 [51264/71234]\n",
      "loss: 0.410398 [57664/71234]\n",
      "loss: 0.542266 [64064/71234]\n",
      "loss: 0.462736 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.122867%, Average loss: 0.436196\n",
      "\n",
      "Epoch 67\n",
      "--------------\n",
      "loss: 0.496976 [   64/71234]\n",
      "loss: 0.386272 [ 6464/71234]\n",
      "loss: 0.348601 [12864/71234]\n",
      "loss: 0.344137 [19264/71234]\n",
      "loss: 0.384118 [25664/71234]\n",
      "loss: 0.613678 [32064/71234]\n",
      "loss: 0.568385 [38464/71234]\n",
      "loss: 0.449204 [44864/71234]\n",
      "loss: 0.451360 [51264/71234]\n",
      "loss: 0.405362 [57664/71234]\n",
      "loss: 0.526324 [64064/71234]\n",
      "loss: 0.457722 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.060631%, Average loss: 0.439247\n",
      "\n",
      "Epoch 68\n",
      "--------------\n",
      "loss: 0.509236 [   64/71234]\n",
      "loss: 0.394047 [ 6464/71234]\n",
      "loss: 0.344721 [12864/71234]\n",
      "loss: 0.343841 [19264/71234]\n",
      "loss: 0.385488 [25664/71234]\n",
      "loss: 0.620053 [32064/71234]\n",
      "loss: 0.565183 [38464/71234]\n",
      "loss: 0.450508 [44864/71234]\n",
      "loss: 0.448874 [51264/71234]\n",
      "loss: 0.406702 [57664/71234]\n",
      "loss: 0.537597 [64064/71234]\n",
      "loss: 0.460114 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.119591%, Average loss: 0.437974\n",
      "\n",
      "Epoch 69\n",
      "--------------\n",
      "loss: 0.507504 [   64/71234]\n",
      "loss: 0.386338 [ 6464/71234]\n",
      "loss: 0.347433 [12864/71234]\n",
      "loss: 0.343459 [19264/71234]\n",
      "loss: 0.383586 [25664/71234]\n",
      "loss: 0.608222 [32064/71234]\n",
      "loss: 0.567287 [38464/71234]\n",
      "loss: 0.448523 [44864/71234]\n",
      "loss: 0.451035 [51264/71234]\n",
      "loss: 0.405602 [57664/71234]\n",
      "loss: 0.533904 [64064/71234]\n",
      "loss: 0.457460 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.077009%, Average loss: 0.437766\n",
      "\n",
      "Epoch 70\n",
      "--------------\n",
      "loss: 0.504026 [   64/71234]\n",
      "loss: 0.394140 [ 6464/71234]\n",
      "loss: 0.344525 [12864/71234]\n",
      "loss: 0.344514 [19264/71234]\n",
      "loss: 0.386266 [25664/71234]\n",
      "loss: 0.618403 [32064/71234]\n",
      "loss: 0.570606 [38464/71234]\n",
      "loss: 0.450128 [44864/71234]\n",
      "loss: 0.450201 [51264/71234]\n",
      "loss: 0.404970 [57664/71234]\n",
      "loss: 0.528782 [64064/71234]\n",
      "loss: 0.458548 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.044253%, Average loss: 0.440129\n",
      "\n",
      "Epoch 71\n",
      "--------------\n",
      "loss: 0.513809 [   64/71234]\n",
      "loss: 0.389197 [ 6464/71234]\n",
      "loss: 0.346283 [12864/71234]\n",
      "loss: 0.342272 [19264/71234]\n",
      "loss: 0.385013 [25664/71234]\n",
      "loss: 0.606691 [32064/71234]\n",
      "loss: 0.564129 [38464/71234]\n",
      "loss: 0.448607 [44864/71234]\n",
      "loss: 0.449634 [51264/71234]\n",
      "loss: 0.406577 [57664/71234]\n",
      "loss: 0.540785 [64064/71234]\n",
      "loss: 0.460863 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.132694%, Average loss: 0.436122\n",
      "\n",
      "Epoch 72\n",
      "--------------\n",
      "loss: 0.496061 [   64/71234]\n",
      "loss: 0.389082 [ 6464/71234]\n",
      "loss: 0.348504 [12864/71234]\n",
      "loss: 0.343878 [19264/71234]\n",
      "loss: 0.385565 [25664/71234]\n",
      "loss: 0.616566 [32064/71234]\n",
      "loss: 0.571837 [38464/71234]\n",
      "loss: 0.450533 [44864/71234]\n",
      "loss: 0.450758 [51264/71234]\n",
      "loss: 0.405423 [57664/71234]\n",
      "loss: 0.523843 [64064/71234]\n",
      "loss: 0.458009 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.995119%, Average loss: 0.440441\n",
      "\n",
      "Epoch 73\n",
      "--------------\n",
      "loss: 0.513237 [   64/71234]\n",
      "loss: 0.392512 [ 6464/71234]\n",
      "loss: 0.344650 [12864/71234]\n",
      "loss: 0.342051 [19264/71234]\n",
      "loss: 0.385626 [25664/71234]\n",
      "loss: 0.615400 [32064/71234]\n",
      "loss: 0.563465 [38464/71234]\n",
      "loss: 0.449110 [44864/71234]\n",
      "loss: 0.448094 [51264/71234]\n",
      "loss: 0.410647 [57664/71234]\n",
      "loss: 0.541200 [64064/71234]\n",
      "loss: 0.462927 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.142520%, Average loss: 0.436006\n",
      "\n",
      "Epoch 74\n",
      "--------------\n",
      "loss: 0.495453 [   64/71234]\n",
      "loss: 0.385977 [ 6464/71234]\n",
      "loss: 0.347941 [12864/71234]\n",
      "loss: 0.343337 [19264/71234]\n",
      "loss: 0.383642 [25664/71234]\n",
      "loss: 0.613643 [32064/71234]\n",
      "loss: 0.568962 [38464/71234]\n",
      "loss: 0.449792 [44864/71234]\n",
      "loss: 0.450703 [51264/71234]\n",
      "loss: 0.405567 [57664/71234]\n",
      "loss: 0.524629 [64064/71234]\n",
      "loss: 0.457754 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 81.024600%, Average loss: 0.439244\n",
      "\n",
      "Epoch 75\n",
      "--------------\n",
      "loss: 0.508720 [   64/71234]\n",
      "loss: 0.363142 [ 6464/71234]\n",
      "loss: 0.350760 [12864/71234]\n",
      "loss: 0.330844 [19264/71234]\n",
      "loss: 0.386719 [25664/71234]\n",
      "loss: 0.636082 [32064/71234]\n",
      "loss: 0.550253 [38464/71234]\n",
      "loss: 0.461561 [44864/71234]\n",
      "loss: 0.445246 [51264/71234]\n",
      "loss: 0.415400 [57664/71234]\n",
      "loss: 0.594246 [64064/71234]\n",
      "loss: 0.476676 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.575846%, Average loss: 0.452866\n",
      "\n",
      "Epoch 76\n",
      "--------------\n",
      "loss: 0.535174 [   64/71234]\n",
      "loss: 0.368531 [ 6464/71234]\n",
      "loss: 0.344877 [12864/71234]\n",
      "loss: 0.342440 [19264/71234]\n",
      "loss: 0.402190 [25664/71234]\n",
      "loss: 0.615302 [32064/71234]\n",
      "loss: 0.551437 [38464/71234]\n",
      "loss: 0.455658 [44864/71234]\n",
      "loss: 0.454718 [51264/71234]\n",
      "loss: 0.418355 [57664/71234]\n",
      "loss: 0.576166 [64064/71234]\n",
      "loss: 0.475398 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.877199%, Average loss: 0.443403\n",
      "\n",
      "Epoch 77\n",
      "--------------\n",
      "loss: 0.519801 [   64/71234]\n",
      "loss: 0.370695 [ 6464/71234]\n",
      "loss: 0.345977 [12864/71234]\n",
      "loss: 0.344309 [19264/71234]\n",
      "loss: 0.403441 [25664/71234]\n",
      "loss: 0.606088 [32064/71234]\n",
      "loss: 0.553519 [38464/71234]\n",
      "loss: 0.452718 [44864/71234]\n",
      "loss: 0.454426 [51264/71234]\n",
      "loss: 0.418631 [57664/71234]\n",
      "loss: 0.561244 [64064/71234]\n",
      "loss: 0.474775 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.513610%, Average loss: 0.443460\n",
      "\n",
      "Epoch 78\n",
      "--------------\n",
      "loss: 0.512821 [   64/71234]\n",
      "loss: 0.372299 [ 6464/71234]\n",
      "loss: 0.347791 [12864/71234]\n",
      "loss: 0.345397 [19264/71234]\n",
      "loss: 0.396079 [25664/71234]\n",
      "loss: 0.599971 [32064/71234]\n",
      "loss: 0.555701 [38464/71234]\n",
      "loss: 0.453910 [44864/71234]\n",
      "loss: 0.453974 [51264/71234]\n",
      "loss: 0.419936 [57664/71234]\n",
      "loss: 0.556370 [64064/71234]\n",
      "loss: 0.475061 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.785483%, Average loss: 0.441709\n",
      "\n",
      "Epoch 79\n",
      "--------------\n",
      "loss: 0.516337 [   64/71234]\n",
      "loss: 0.369353 [ 6464/71234]\n",
      "loss: 0.346442 [12864/71234]\n",
      "loss: 0.346565 [19264/71234]\n",
      "loss: 0.387935 [25664/71234]\n",
      "loss: 0.597483 [32064/71234]\n",
      "loss: 0.557556 [38464/71234]\n",
      "loss: 0.453547 [44864/71234]\n",
      "loss: 0.453200 [51264/71234]\n",
      "loss: 0.421230 [57664/71234]\n",
      "loss: 0.553210 [64064/71234]\n",
      "loss: 0.475374 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.428445%, Average loss: 0.442842\n",
      "\n",
      "Epoch 80\n",
      "--------------\n",
      "loss: 0.511714 [   64/71234]\n",
      "loss: 0.370370 [ 6464/71234]\n",
      "loss: 0.347013 [12864/71234]\n",
      "loss: 0.344210 [19264/71234]\n",
      "loss: 0.384941 [25664/71234]\n",
      "loss: 0.597900 [32064/71234]\n",
      "loss: 0.559527 [38464/71234]\n",
      "loss: 0.453239 [44864/71234]\n",
      "loss: 0.453768 [51264/71234]\n",
      "loss: 0.420603 [57664/71234]\n",
      "loss: 0.550289 [64064/71234]\n",
      "loss: 0.476421 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.569295%, Average loss: 0.441635\n",
      "\n",
      "Epoch 81\n",
      "--------------\n",
      "loss: 0.513952 [   64/71234]\n",
      "loss: 0.373099 [ 6464/71234]\n",
      "loss: 0.343038 [12864/71234]\n",
      "loss: 0.343316 [19264/71234]\n",
      "loss: 0.389798 [25664/71234]\n",
      "loss: 0.597091 [32064/71234]\n",
      "loss: 0.561874 [38464/71234]\n",
      "loss: 0.452403 [44864/71234]\n",
      "loss: 0.451392 [51264/71234]\n",
      "loss: 0.419313 [57664/71234]\n",
      "loss: 0.547610 [64064/71234]\n",
      "loss: 0.474367 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.713420%, Average loss: 0.440265\n",
      "\n",
      "Epoch 82\n",
      "--------------\n",
      "loss: 0.513092 [   64/71234]\n",
      "loss: 0.373045 [ 6464/71234]\n",
      "loss: 0.341879 [12864/71234]\n",
      "loss: 0.339447 [19264/71234]\n",
      "loss: 0.384506 [25664/71234]\n",
      "loss: 0.597799 [32064/71234]\n",
      "loss: 0.562389 [38464/71234]\n",
      "loss: 0.452596 [44864/71234]\n",
      "loss: 0.450756 [51264/71234]\n",
      "loss: 0.419862 [57664/71234]\n",
      "loss: 0.547483 [64064/71234]\n",
      "loss: 0.474344 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.841167%, Average loss: 0.439069\n",
      "\n",
      "Epoch 83\n",
      "--------------\n",
      "loss: 0.511906 [   64/71234]\n",
      "loss: 0.370738 [ 6464/71234]\n",
      "loss: 0.343099 [12864/71234]\n",
      "loss: 0.340538 [19264/71234]\n",
      "loss: 0.384017 [25664/71234]\n",
      "loss: 0.598519 [32064/71234]\n",
      "loss: 0.563063 [38464/71234]\n",
      "loss: 0.453658 [44864/71234]\n",
      "loss: 0.451256 [51264/71234]\n",
      "loss: 0.419846 [57664/71234]\n",
      "loss: 0.548652 [64064/71234]\n",
      "loss: 0.474225 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.624980%, Average loss: 0.440690\n",
      "\n",
      "Epoch 84\n",
      "--------------\n",
      "loss: 0.513850 [   64/71234]\n",
      "loss: 0.376049 [ 6464/71234]\n",
      "loss: 0.343655 [12864/71234]\n",
      "loss: 0.341327 [19264/71234]\n",
      "loss: 0.384226 [25664/71234]\n",
      "loss: 0.597395 [32064/71234]\n",
      "loss: 0.563768 [38464/71234]\n",
      "loss: 0.453745 [44864/71234]\n",
      "loss: 0.451597 [51264/71234]\n",
      "loss: 0.420632 [57664/71234]\n",
      "loss: 0.547754 [64064/71234]\n",
      "loss: 0.472607 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.585673%, Average loss: 0.441474\n",
      "\n",
      "Epoch 85\n",
      "--------------\n",
      "loss: 0.515141 [   64/71234]\n",
      "loss: 0.376203 [ 6464/71234]\n",
      "loss: 0.342230 [12864/71234]\n",
      "loss: 0.338808 [19264/71234]\n",
      "loss: 0.380303 [25664/71234]\n",
      "loss: 0.599226 [32064/71234]\n",
      "loss: 0.561901 [38464/71234]\n",
      "loss: 0.453314 [44864/71234]\n",
      "loss: 0.451328 [51264/71234]\n",
      "loss: 0.419637 [57664/71234]\n",
      "loss: 0.544193 [64064/71234]\n",
      "loss: 0.472767 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.602051%, Average loss: 0.441074\n",
      "\n",
      "Epoch 86\n",
      "--------------\n",
      "loss: 0.513961 [   64/71234]\n",
      "loss: 0.371859 [ 6464/71234]\n",
      "loss: 0.346004 [12864/71234]\n",
      "loss: 0.339670 [19264/71234]\n",
      "loss: 0.381317 [25664/71234]\n",
      "loss: 0.598476 [32064/71234]\n",
      "loss: 0.562423 [38464/71234]\n",
      "loss: 0.452399 [44864/71234]\n",
      "loss: 0.450481 [51264/71234]\n",
      "loss: 0.420957 [57664/71234]\n",
      "loss: 0.541406 [64064/71234]\n",
      "loss: 0.472107 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.592224%, Average loss: 0.441071\n",
      "\n",
      "Epoch 87\n",
      "--------------\n",
      "loss: 0.513445 [   64/71234]\n",
      "loss: 0.371004 [ 6464/71234]\n",
      "loss: 0.342692 [12864/71234]\n",
      "loss: 0.337771 [19264/71234]\n",
      "loss: 0.382396 [25664/71234]\n",
      "loss: 0.599788 [32064/71234]\n",
      "loss: 0.561782 [38464/71234]\n",
      "loss: 0.453379 [44864/71234]\n",
      "loss: 0.447077 [51264/71234]\n",
      "loss: 0.421665 [57664/71234]\n",
      "loss: 0.545994 [64064/71234]\n",
      "loss: 0.473903 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.854270%, Average loss: 0.438138\n",
      "\n",
      "Epoch 88\n",
      "--------------\n",
      "loss: 0.513413 [   64/71234]\n",
      "loss: 0.372614 [ 6464/71234]\n",
      "loss: 0.344496 [12864/71234]\n",
      "loss: 0.338861 [19264/71234]\n",
      "loss: 0.379387 [25664/71234]\n",
      "loss: 0.598067 [32064/71234]\n",
      "loss: 0.564359 [38464/71234]\n",
      "loss: 0.452530 [44864/71234]\n",
      "loss: 0.448782 [51264/71234]\n",
      "loss: 0.421238 [57664/71234]\n",
      "loss: 0.541327 [64064/71234]\n",
      "loss: 0.471611 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.582397%, Average loss: 0.441050\n",
      "\n",
      "Epoch 89\n",
      "--------------\n",
      "loss: 0.511420 [   64/71234]\n",
      "loss: 0.373985 [ 6464/71234]\n",
      "loss: 0.344135 [12864/71234]\n",
      "loss: 0.336864 [19264/71234]\n",
      "loss: 0.379313 [25664/71234]\n",
      "loss: 0.598494 [32064/71234]\n",
      "loss: 0.564515 [38464/71234]\n",
      "loss: 0.452748 [44864/71234]\n",
      "loss: 0.449071 [51264/71234]\n",
      "loss: 0.421711 [57664/71234]\n",
      "loss: 0.542720 [64064/71234]\n",
      "loss: 0.472233 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.585673%, Average loss: 0.440732\n",
      "\n",
      "Epoch 90\n",
      "--------------\n",
      "loss: 0.512560 [   64/71234]\n",
      "loss: 0.373928 [ 6464/71234]\n",
      "loss: 0.343734 [12864/71234]\n",
      "loss: 0.339792 [19264/71234]\n",
      "loss: 0.379191 [25664/71234]\n",
      "loss: 0.601222 [32064/71234]\n",
      "loss: 0.564683 [38464/71234]\n",
      "loss: 0.452404 [44864/71234]\n",
      "loss: 0.447198 [51264/71234]\n",
      "loss: 0.421635 [57664/71234]\n",
      "loss: 0.544720 [64064/71234]\n",
      "loss: 0.472135 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.638082%, Average loss: 0.440532\n",
      "\n",
      "Epoch 91\n",
      "--------------\n",
      "loss: 0.513059 [   64/71234]\n",
      "loss: 0.373132 [ 6464/71234]\n",
      "loss: 0.345509 [12864/71234]\n",
      "loss: 0.338830 [19264/71234]\n",
      "loss: 0.377038 [25664/71234]\n",
      "loss: 0.600682 [32064/71234]\n",
      "loss: 0.564859 [38464/71234]\n",
      "loss: 0.452780 [44864/71234]\n",
      "loss: 0.448077 [51264/71234]\n",
      "loss: 0.422148 [57664/71234]\n",
      "loss: 0.543268 [64064/71234]\n",
      "loss: 0.471267 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.693767%, Average loss: 0.439828\n",
      "\n",
      "Epoch 92\n",
      "--------------\n",
      "loss: 0.514617 [   64/71234]\n",
      "loss: 0.373174 [ 6464/71234]\n",
      "loss: 0.344278 [12864/71234]\n",
      "loss: 0.339188 [19264/71234]\n",
      "loss: 0.378366 [25664/71234]\n",
      "loss: 0.601220 [32064/71234]\n",
      "loss: 0.567197 [38464/71234]\n",
      "loss: 0.452001 [44864/71234]\n",
      "loss: 0.450964 [51264/71234]\n",
      "loss: 0.420733 [57664/71234]\n",
      "loss: 0.542430 [64064/71234]\n",
      "loss: 0.471830 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.680664%, Average loss: 0.440450\n",
      "\n",
      "Epoch 93\n",
      "--------------\n",
      "loss: 0.514031 [   64/71234]\n",
      "loss: 0.371302 [ 6464/71234]\n",
      "loss: 0.344427 [12864/71234]\n",
      "loss: 0.339095 [19264/71234]\n",
      "loss: 0.377591 [25664/71234]\n",
      "loss: 0.601104 [32064/71234]\n",
      "loss: 0.565763 [38464/71234]\n",
      "loss: 0.452143 [44864/71234]\n",
      "loss: 0.451914 [51264/71234]\n",
      "loss: 0.420761 [57664/71234]\n",
      "loss: 0.545038 [64064/71234]\n",
      "loss: 0.470761 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.775656%, Average loss: 0.439190\n",
      "\n",
      "Epoch 94\n",
      "--------------\n",
      "loss: 0.515545 [   64/71234]\n",
      "loss: 0.372916 [ 6464/71234]\n",
      "loss: 0.344189 [12864/71234]\n",
      "loss: 0.338330 [19264/71234]\n",
      "loss: 0.377696 [25664/71234]\n",
      "loss: 0.602445 [32064/71234]\n",
      "loss: 0.565080 [38464/71234]\n",
      "loss: 0.452251 [44864/71234]\n",
      "loss: 0.449554 [51264/71234]\n",
      "loss: 0.420773 [57664/71234]\n",
      "loss: 0.543155 [64064/71234]\n",
      "loss: 0.471823 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.733073%, Average loss: 0.439906\n",
      "\n",
      "Epoch 95\n",
      "--------------\n",
      "loss: 0.513157 [   64/71234]\n",
      "loss: 0.372472 [ 6464/71234]\n",
      "loss: 0.343664 [12864/71234]\n",
      "loss: 0.338661 [19264/71234]\n",
      "loss: 0.377788 [25664/71234]\n",
      "loss: 0.600450 [32064/71234]\n",
      "loss: 0.567564 [38464/71234]\n",
      "loss: 0.450953 [44864/71234]\n",
      "loss: 0.450894 [51264/71234]\n",
      "loss: 0.422163 [57664/71234]\n",
      "loss: 0.542403 [64064/71234]\n",
      "loss: 0.471152 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.873923%, Average loss: 0.437916\n",
      "\n",
      "Epoch 96\n",
      "--------------\n",
      "loss: 0.512854 [   64/71234]\n",
      "loss: 0.371915 [ 6464/71234]\n",
      "loss: 0.343649 [12864/71234]\n",
      "loss: 0.338570 [19264/71234]\n",
      "loss: 0.376169 [25664/71234]\n",
      "loss: 0.600241 [32064/71234]\n",
      "loss: 0.566260 [38464/71234]\n",
      "loss: 0.451361 [44864/71234]\n",
      "loss: 0.452777 [51264/71234]\n",
      "loss: 0.421958 [57664/71234]\n",
      "loss: 0.543144 [64064/71234]\n",
      "loss: 0.470695 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.821514%, Average loss: 0.438360\n",
      "\n",
      "Epoch 97\n",
      "--------------\n",
      "loss: 0.512639 [   64/71234]\n",
      "loss: 0.371911 [ 6464/71234]\n",
      "loss: 0.344027 [12864/71234]\n",
      "loss: 0.338460 [19264/71234]\n",
      "loss: 0.375564 [25664/71234]\n",
      "loss: 0.601444 [32064/71234]\n",
      "loss: 0.567202 [38464/71234]\n",
      "loss: 0.451899 [44864/71234]\n",
      "loss: 0.452145 [51264/71234]\n",
      "loss: 0.422465 [57664/71234]\n",
      "loss: 0.540881 [64064/71234]\n",
      "loss: 0.470762 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.828065%, Average loss: 0.438841\n",
      "\n",
      "Epoch 98\n",
      "--------------\n",
      "loss: 0.510172 [   64/71234]\n",
      "loss: 0.371866 [ 6464/71234]\n",
      "loss: 0.346340 [12864/71234]\n",
      "loss: 0.336098 [19264/71234]\n",
      "loss: 0.376236 [25664/71234]\n",
      "loss: 0.602030 [32064/71234]\n",
      "loss: 0.568003 [38464/71234]\n",
      "loss: 0.452091 [44864/71234]\n",
      "loss: 0.452152 [51264/71234]\n",
      "loss: 0.421720 [57664/71234]\n",
      "loss: 0.543952 [64064/71234]\n",
      "loss: 0.470748 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.795309%, Average loss: 0.438579\n",
      "\n",
      "Epoch 99\n",
      "--------------\n",
      "loss: 0.511213 [   64/71234]\n",
      "loss: 0.371387 [ 6464/71234]\n",
      "loss: 0.343334 [12864/71234]\n",
      "loss: 0.336830 [19264/71234]\n",
      "loss: 0.377582 [25664/71234]\n",
      "loss: 0.603016 [32064/71234]\n",
      "loss: 0.568761 [38464/71234]\n",
      "loss: 0.451136 [44864/71234]\n",
      "loss: 0.450706 [51264/71234]\n",
      "loss: 0.422351 [57664/71234]\n",
      "loss: 0.541021 [64064/71234]\n",
      "loss: 0.470019 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.772380%, Average loss: 0.438781\n",
      "\n",
      "Epoch 100\n",
      "--------------\n",
      "loss: 0.513719 [   64/71234]\n",
      "loss: 0.372345 [ 6464/71234]\n",
      "loss: 0.343980 [12864/71234]\n",
      "loss: 0.336067 [19264/71234]\n",
      "loss: 0.377379 [25664/71234]\n",
      "loss: 0.604498 [32064/71234]\n",
      "loss: 0.569120 [38464/71234]\n",
      "loss: 0.450002 [44864/71234]\n",
      "loss: 0.453214 [51264/71234]\n",
      "loss: 0.423140 [57664/71234]\n",
      "loss: 0.543108 [64064/71234]\n",
      "loss: 0.471083 [70464/71234]\n",
      "Test Error: \n",
      " Accuracy: 80.782207%, Average loss: 0.438635\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-3\n",
    "epochs = 100\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = NeuralNetwork()\n",
    "swa_start = 75\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
    "swa_model = torch.optim.swa_utils.AveragedModel(model)\n",
    "swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \\\n",
    "                            anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=0.05)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n--------------\")\n",
    "    train_loop(dataloader=train_dataloader, \n",
    "                model=model, \n",
    "                swa_start=0.75*epochs, \n",
    "                swa_model=swa_model, \n",
    "                scheduler=scheduler,\n",
    "                swa_scheduler=swa_scheduler,\n",
    "                epoch=t+1,\n",
    "                loss_fn=loss_fn,\n",
    "                optimizer=optimizer)\n",
    "    test_loop(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
